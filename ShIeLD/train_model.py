#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SHIELD training script (train/val on precomputed graphs).

This script trains the ShIeLD model on graph datasets previously generated by the
graph-creation pipeline (e.g., Voronoi or bucket segmentation). It performs
cross-validation style training by selecting one split as validation and using
the remaining splits for training.

Core responsibilities
---------------------
1) Load a requirements dictionary (pickled) that defines:
   - model hyperparameters (layers, dropout, learning rate, etc.)
   - dataset locations (path_to_data_set)
   - graph construction meta-parameters to iterate over (radii, anchors, etc.)
   - batching, normalization settings, and label mapping

2) Enumerate training configurations across:
   - radius_distance (requirements["radius_distance_all"])
   - anker_number / anchor value (requirements["anker_value_all"])
   - fussy_limit (Voronoi only; requirements["fussy_limit_all"], or a bucket sentinel)
   - dropout rates (requirements["dropout_rate"])
   - repeated training runs (args.number_of_training_repeats)
   - a chosen split_number used as validation fold (args.split_number)

3) For each configuration:
   - Load train/validation graph lists via `graph_dataset` and DataListLoader.
   - Initialize the loss function (class-balanced / weighted, optionally noisy labels).
   - Construct a ShIeLD model instance.
   - Train using `train_utils.train_loop_shield` (with early stopping patience).
   - Evaluate on train and validation loaders via `model_utils.get_acc_metrics`.
   - Append results to the central CSV (via train_utils.get_train_results_csv).

Inputs
------
- requirements_file_path:
    Pickle file containing a dict of dataset + model/training settings.
- Graphs on disk:
    Expected directory structure under requirements["path_to_data_set"], encoding:
      * anker value
      * minimum number of cells
      * fussy limit (Voronoi) OR bucket_sampling folder (Random)
      * radius distance
    and inside that:
      train/graphs/
      (plus file name lists like `train_set_validation_split_{split}_file_names.pkl`)

Outputs
-------
- A results CSV written/managed by train_utils.get_train_results_csv, containing
  metadata columns such as anchor value, radius, fussy limit, dropout, split,
  and performance metrics (balanced accuracy, F1, etc.).

Parallelism & system settings
-----------------------------
- Sets torch multiprocessing start method to "fork" and sharing strategy to
  "file_system".
- Uses DataListLoader with num_workers=8 and prefetch_factor=50.

Notes / caveats
---------------
- The script currently imports some modules that are not used in this file
  (e.g., model_utils/train_utils are used, but data_utils is only used for
  bool parsing; DataListLoader is used; some other imports may be legacy).
- Paths for loss initialization differ when noise_yLabel is enabled.
- This script assumes that graph generation has already happened and that
  file name lists for train/validation splits exist in the expected locations.

Created Nov 2024

@author: Vivek
"""

import argparse
import pickle
from pathlib import Path
import torch
import pandas as pd
import multiprocessing as mp
from tqdm import tqdm

from torch_geometric.loader import DataListLoader
from .utils.data_class import graph_dataset
from .utils import train_utils
from .utils import model_utils
from .utils import data_utils

from .tests import input_test
from itertools import product
from .model import ShIeLD


# Configure torch multiprocessing.
# - "fork" is often faster on Linux but can be fragile with CUDA contexts;
#   this is the current choice in your script.
torch.multiprocessing.set_start_method("fork", force=True)

# Use filesystem-based tensor sharing to avoid shared-memory limits on large runs.
torch.multiprocessing.set_sharing_strategy("file_system")


# Select compute device (GPU if available).
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def main():
    # -----------------------------
    # CLI / configuration
    # -----------------------------
    parser = argparse.ArgumentParser()

    # Which CV split is used as validation fold (the remaining folds become training folds).
    parser.add_argument("-split", "--split_number", type=int, default=2)

    # Repeat each configuration multiple times (different random seeds / stochastic training).
    parser.add_argument("-rep", "--number_of_training_repeats", type=int, default=5)

    # Free-form comment string stored in the results CSV (useful for tracking experiments).
    parser.add_argument("-c", "--comment", type=str, default="noVoro")

    # Path to requirements dict (pickle file).
    parser.add_argument(
        "-req_path",
        "--requirements_file_path",
        default=Path.cwd() / "examples" / "CRC" / "requirements.pt",
    )

    # Optional perturbations / ablations.
    # Note: these are passed into model/loss initialization later.
    parser.add_argument("-noisy_edge", "--noisy_edge", default=False)
    parser.add_argument("-noise_yLabel", "--noise_yLabel", default=False)

    # Convenience option: reverse iteration order over radii (debugging).
    parser.add_argument("-rev", "--reverse_sampling", default=False)

    parser.add_argument(
        "--verbose", action="store_true", help="Enable verbose logging."
    )

    args = parser.parse_args()

    # Normalize boolean-like CLI input to a real bool.
    # (Ensure Bool Parameters are treated as bool:)
    args.reverse_sampling = data_utils.bool_passer(args.reverse_sampling)
    args.noise_yLabel = data_utils.bool_passer(args.noise_yLabel)
    args.noisy_edge = data_utils.bool_passer(args.noisy_edge)

    # -----------------------------
    # Load and validate requirements
    # -----------------------------
    requirements = pickle.load(open(Path.cwd() / args.requirements_file_path, "rb"))

    # Validate requirements schema/keys expected by the pipeline.
    requirements = input_test.validate_all_keys_in_req(
        req_file=requirements, verbose=args.verbose
    )

    if args.verbose:
        print(args, flush=True)
        print("Device:", device)

    # -----------------------------
    # Training patience (early stopping)
    # -----------------------------
    # If patience is specified in requirements, use it; otherwise default to 5.
    if "patience" in requirements.keys():
        patience = requirements["patience"]
    else:
        patience = 9

    # -----------------------------
    # Determine sampling mode: Voronoi vs bucket
    # -----------------------------
    # If there were no Voronoi regions constructed, iterating fussy_limit makes no sense.
    # The folder structure differs depending on sampling strategy.
    if "sampling" in requirements.keys():
        if requirements["sampling"] == "bucket":
            # Random sampling: there is no true fussy_limit dimension; use a sentinel.
            fussy_vector = ["bucketSampling"]
            fussy_folder_name = "bucket_sampling"
        elif requirements["sampling"] == "voronoi":
            # Voronoi sampling: fussy_limit is a real meta-parameter.
            fussy_folder_name = True
            fussy_vector = requirements["fussy_limit_all"]
    else:
        # Backwards compatibility: default assumes Voronoi-like layout.
        fussy_vector = requirements["fussy_limit_all"]
        fussy_folder_name = True

    split_number = int(args.split_number)

    # Load (or create) the training results CSV and the path where it is stored.
    training_results_csv, csv_file_path = train_utils.get_train_results_csv(
        requirement_dict=requirements
    )

    # Metadata columns used to detect if a run has already been trained.
    meta_columns = [
        "anker_value",
        "radius_distance",
        "fussy_limit",
        "dropout_rate",
        "comment",
        "comment_norm",
        "model_no",
        "split_number",
    ]

    # Iterate radii (optionally reversed).
    radii = requirements["radius_distance_all"]
    if args.reverse_sampling:
        radii = radii[::-1]

    # -----------------------------
    # Sweep across graph meta-parameters
    # -----------------------------
    for radius_distance, fussy_limit, anker_number in product(
        radii, fussy_vector, requirements["anker_value_all"]
    ):
        # Construct the directory holding the graphs for this configuration.
        # For Voronoi: includes fussy_limit_<...> in path.
        # For bucket: uses the bucket_sampling folder instead.
        if fussy_folder_name is True:
            path_to_graphs = Path(
                requirements["path_to_data_set"]
                / f"anker_value_{anker_number}".replace(".", "_")
                / f"min_cells_{requirements['minimum_number_cells']}"
                / f"fussy_limit_{fussy_limit}".replace(".", "_")
                / f"radius_{radius_distance}"
            )
        else:
            path_to_graphs = Path(
                requirements["path_to_data_set"]
                / f"anker_value_{anker_number}".replace(".", "_")
                / f"min_cells_{requirements['minimum_number_cells']}"
                / f"{fussy_folder_name}"
                / f"radius_{radius_distance}"
            )

        # Proactively clear CUDA cache between configurations (reduces OOM risk).
        torch.cuda.empty_cache()

        # Determine training folds: all validation splits except the chosen split_number.
        train_folds = requirements["number_validation_splits"].copy()
        train_folds.remove(split_number)

        # Optional dataset-based normalization:
        # - if requirements["databased_norm"] is not None: load/save a normalizer
        #   keyed to the validation split.
        if requirements["databased_norm"] is not None:
            databased_norm = requirements["databased_norm"]
            file_name_data_norm = (
                f"train_set_validation_split_{split_number}_standadizer.pkl"
            )
        else:
            databased_norm = None
            file_name_data_norm = None

        # -----------------------------
        # Data loaders (train + validation)
        # -----------------------------
        # DataListLoader loads batches as lists of PyG Data objects (useful for DataParallel).
        #

        data_loader_train = DataListLoader(
            graph_dataset(
                root=str(path_to_graphs / "train" / "graphs"),
                path_to_graphs=path_to_graphs,
                fold_ids=train_folds,
                requirements_dict=requirements,
                graph_file_names=f"train_set_validation_split_{split_number}_file_names.pkl",
                normalize=databased_norm,
                normalizer_filename=file_name_data_norm,
                verbose=args.verbose,
            ),
            batch_size=requirements["batch_size"],
            shuffle=True,
            num_workers=data_utils.get_number_cpuWorkers(),
            prefetch_factor=50,
        )

        data_loader_validation = DataListLoader(
            graph_dataset(
                root=str(path_to_graphs / "train" / "graphs"),
                path_to_graphs=path_to_graphs,
                fold_ids=[split_number],
                requirements_dict=requirements,
                graph_file_names=f"validation_validation_split_{split_number}_file_names.pkl",
                normalize=databased_norm,
                normalizer_filename=file_name_data_norm,
                verbose=args.verbose,
            ),
            batch_size=requirements["batch_size"],
            shuffle=True,
            num_workers=data_utils.get_number_cpuWorkers(),
            prefetch_factor=50,
        )

        # -----------------------------
        # Sweep dropout rates and training repeats
        # -----------------------------
        if args.verbose:
            print("started training:")
        for dp, num in tqdm(
            product(
                requirements["dropout_rate"],
                range(int(args.number_of_training_repeats)),
            ),
            total=len(requirements["dropout_rate"])
            * int(args.number_of_training_repeats),
        ):
            # Values that uniquely define this run (used for deduping via CSV).
            row_values = [
                anker_number,
                radius_distance,
                fussy_limit,
                dp,
                args.comment,
                requirements["comment_norm"],
                num,
                split_number,
            ]

            # Check whether this exact configuration already exists in the CSV.
            filter_match = (training_results_csv[meta_columns] == row_values).all(
                axis=1
            )

            if not filter_match.any():
                # -----------------------------------------
                # Loss function initialization
                # -----------------------------------------
                # NOTE: loss_init_path depends on whether noise_yLabel is enabled.
                # The conditional expression here is exactly as in your script.
                loss_init_path = (
                    path_to_graphs / "train" / "graphs"
                    if args.noise_yLabel is not False
                    else path_to_graphs
                    / f"train_set_validation_split_{split_number}_file_names.pkl"
                )

                loss_fkt = train_utils.initialize_loss(
                    path=Path(loss_init_path),
                    tissue_dict=requirements["label_dict"],
                    device=device,
                    noise_yLabel=args.noise_yLabel,
                )

                # -----------------------------------------
                # Model initialization
                # -----------------------------------------
                model = ShIeLD(
                    num_of_feat=int(requirements["input_layer"]),
                    layer_1=requirements["layer_1"],
                    layer_final=requirements["output_layer"],
                    dp=dp,
                    self_att=False,
                    attr_bool=requirements["attr_bool"],
                    norm_type=requirements["comment_norm"],
                    noisy_edge=args.noisy_edge,
                ).to(device)

                # Switch to training mode (enables dropout, etc.).
                model.train()

                # -----------------------------------------
                # Training loop
                # -----------------------------------------
                model, train_loss = train_utils.train_loop_shield(
                    optimizer=torch.optim.Adam(
                        model.parameters(), lr=requirements["learning_rate"]
                    ),
                    model=model,
                    data_loader=data_loader_train,
                    loss_fkt=loss_fkt,
                    attr_bool=requirements["attr_bool"],
                    device=device,
                    patience=patience,
                    noise_yLabel=args.noise_yLabel,
                )

                # -----------------------------------------
                # Evaluation (train + validation)
                # -----------------------------------------
                model.eval()
                if args.verbose:
                    print("start validation")

                # Compute metrics on training set.
                train_bal_acc, train_f1_score, train_cm = model_utils.get_acc_metrics(
                    model=model,
                    data_loader=data_loader_train,
                    device=device,
                )

                # Compute metrics on validation set.
                val_bal_acc, val_f1_score, test_cm = model_utils.get_acc_metrics(
                    model=model,
                    data_loader=data_loader_validation,
                    device=device,
                )

                # -----------------------------------------
                # Persist results to CSV
                # -----------------------------------------
                model_csv = pd.DataFrame(
                    [
                        [
                            anker_number,
                            radius_distance,
                            fussy_limit,
                            dp,
                            args.comment,
                            requirements["comment_norm"],
                            num,
                            train_bal_acc,
                            train_f1_score,
                            val_bal_acc,
                            val_f1_score,
                            split_number,
                        ]
                    ],
                    columns=training_results_csv.columns,
                )
                if args.verbose:
                    print("train_bal_acc", "train_f1_score")
                    print(train_bal_acc, train_f1_score)
                    print("val_bal_acc", "val_f1_score")
                    print(val_bal_acc, val_f1_score)

                # Reload CSV (in case other processes appended, or file changed on disk).
                training_results_csv, csv_file_path = train_utils.get_train_results_csv(
                    requirement_dict=requirements
                )

                # Prepend the new row (model_csv) to the existing results.
                training_results_csv = pd.concat(
                    [model_csv, training_results_csv], ignore_index=True
                )

                # Write out updated results.
                training_results_csv.to_csv(csv_file_path, index=False)

                # Clear CUDA cache after finishing a run.
                torch.cuda.empty_cache()
            else:
                # Skip training if the exact configuration already exists.
                if args.verbose:
                    print(
                        "Model already trained:",
                        anker_number,
                        radius_distance,
                        fussy_limit,
                        dp,
                        args.comment,
                        requirements["comment_norm"],
                        num,
                    )


if __name__ == "__main__":
    main()
